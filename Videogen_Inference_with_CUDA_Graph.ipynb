{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8XnmMveljMUf"
      },
      "outputs": [],
      "source": [
        "# 1. Install necessary libraries\n",
        "!pip install diffusers transformers accelerate huggingface_hub\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e143M5ETjPGj"
      },
      "outputs": [],
      "source": [
        "# 2. Imports\n",
        "import torch\n",
        "from torch import autocast\n",
        "from diffusers import DiffusionPipeline\n",
        "from huggingface_hub import snapshot_download\n",
        "import gc"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S1Gk5CC2vVZc"
      },
      "outputs": [],
      "source": [
        "import pynvml\n",
        "import torch\n",
        "\n",
        "# Initialize NVML once at the start of your notebook\n",
        "pynvml.nvmlInit()\n",
        "\n",
        "def get_gpu_metrics(device_idx=0):\n",
        "    \"\"\"\n",
        "    Print GPU memory usage, SM utilization, and memory bandwidth utilization.\n",
        "    \"\"\"\n",
        "    # PyTorch memory metrics\n",
        "    used_mem_MB = torch.cuda.memory_allocated(device_idx) / 1024 / 1024\n",
        "    reserved_mem_MB = torch.cuda.memory_reserved(device_idx) / 1024 / 1024\n",
        "    total_mem_MB = torch.cuda.get_device_properties(device_idx).total_memory / 1024 / 1024\n",
        "\n",
        "    # NVML metrics\n",
        "    handle = pynvml.nvmlDeviceGetHandleByIndex(device_idx)\n",
        "    mem_info = pynvml.nvmlDeviceGetMemoryInfo(handle)\n",
        "    util_info = pynvml.nvmlDeviceGetUtilizationRates(handle)\n",
        "\n",
        "    metrics = {\n",
        "        \"pytorch_allocated_MB\": used_mem_MB,\n",
        "        \"pytorch_reserved_MB\": reserved_mem_MB,\n",
        "        \"nvml_used_MB\": mem_info.used / 1024 / 1024,\n",
        "        \"total_memory_MB\": total_mem_MB,\n",
        "        \"memory_utilization_percent\": 100 * used_mem_MB / total_mem_MB,\n",
        "        \"sm_utilization_percent\": util_info.gpu,\n",
        "        \"memory_bandwidth_utilization_percent\": util_info.memory\n",
        "    }\n",
        "\n",
        "    return metrics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O7KVXM93jQnJ"
      },
      "outputs": [],
      "source": [
        "# 3. Download and Load Model\n",
        "model_path = snapshot_download(repo_id=\"cerspense/zeroscope_v2_576w\")\n",
        "\n",
        "pipe = DiffusionPipeline.from_pretrained(\n",
        "    model_path,\n",
        "    torch_dtype=torch.float16\n",
        ").to(\"cuda\")\n",
        "\n",
        "pipe.safety_checker = lambda images, **kwargs: (images, [False] * len(images))  # Disable safety checker\n",
        "pipe.unet.eval()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o1FFO9vCr8nX"
      },
      "outputs": [],
      "source": [
        "batch_size = 1\n",
        "num_frames = 8"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CVPR9owhlFaY"
      },
      "outputs": [],
      "source": [
        "\n",
        "prompt_list_full = [\n",
        "    \"A cinematic sunset over the mountains\",\n",
        "    \"A bustling city skyline at night\",\n",
        "    \"A peaceful forest in autumn\",\n",
        "    \"A futuristic space station orbiting a planet\",\n",
        "    \"A snowy village during Christmas\",\n",
        "    \"A tropical beach at sunrise\",\n",
        "    \"An ancient castle on a misty hill\",\n",
        "    \"A colorful coral reef underwater\",\n",
        "    \"A cute cat\",\n",
        "    \"Dancing cat video\",\n",
        "    \"Harry Potter\",\n",
        "    \"A cute european village\",\n",
        "]\n",
        "\n",
        "prompt_list = prompt_list_full[:batch_size]\n",
        "\n",
        "# Get tokenizer and text_encoder from pipeline\n",
        "tokenizer = pipe.tokenizer\n",
        "text_encoder = pipe.text_encoder\n",
        "\n",
        "# Tokenize prompt\n",
        "text_inputs = tokenizer(\n",
        "    prompt_list,\n",
        "    padding=\"max_length\",\n",
        "    max_length=tokenizer.model_max_length,\n",
        "    return_tensors=\"pt\"\n",
        ")\n",
        "\n",
        "input_ids = text_inputs.input_ids.to(\"cuda\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FUwjzW6slijJ"
      },
      "outputs": [],
      "source": [
        "# Encode text\n",
        "with torch.no_grad():\n",
        "    encoder_hidden_states = text_encoder(input_ids)[0]  # (batch_size, seq_len, hidden_dim)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9geqoQRDllNq"
      },
      "outputs": [],
      "source": [
        "# 5. Prepare Static Latents and Timestep\n",
        "latent_shape = (batch_size, 4, num_frames, 64, 64)\n",
        "\n",
        "latents = torch.randn(latent_shape, device=\"cuda\", dtype=torch.float16)\n",
        "timestep = torch.tensor([50], device=\"cuda\", dtype=torch.float16)  # Random timestep\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cgX8ECHH6frP"
      },
      "outputs": [],
      "source": [
        "# Torch Compile\n",
        "# This one line decreased the inference time by 3x\n",
        "pipe.unet = torch.compile(pipe.unet)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_WylwENEln1Z"
      },
      "outputs": [],
      "source": [
        "# 6. Warmup UNet Forward\n",
        "with torch.no_grad(), autocast(\"cuda\"):\n",
        "    _ = pipe.unet(latents, timestep, encoder_hidden_states).sample\n",
        "\n",
        "print(\"Warmup UNet Forward\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0U49DWpplqTg"
      },
      "outputs": [],
      "source": [
        "# 7. Capture UNet forward with CUDA Graph\n",
        "# Graph capture increased SM utilization to almost 100%\n",
        "print(\"Capturing UNet with CUDA Graph...\")\n",
        "\n",
        "graph = torch.cuda.CUDAGraph()\n",
        "\n",
        "with torch.cuda.graph(graph):\n",
        "    with torch.no_grad(), autocast(\"cuda\"):\n",
        "        unet_output = pipe.unet(latents, timestep, encoder_hidden_states).sample\n",
        "\n",
        "torch.cuda.synchronize()\n",
        "print(\"Capture complete!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4zyU2mEllubq"
      },
      "outputs": [],
      "source": [
        "# 8. Replay and Profile\n",
        "profile_logdir = \"./graph_profile_log\"\n",
        "\n",
        "print(\"Profiling CUDA Graph replays...\")\n",
        "\n",
        "start_event = torch.cuda.Event(enable_timing=True)\n",
        "end_event = torch.cuda.Event(enable_timing=True)\n",
        "\n",
        "total_time_ms = 0.0\n",
        "num_repeats = 10\n",
        "\n",
        "# Warmup replay\n",
        "graph.replay()\n",
        "torch.cuda.synchronize()\n",
        "\n",
        "metrics_per_step = []\n",
        "\n",
        "with torch.profiler.profile(\n",
        "    activities=[torch.profiler.ProfilerActivity.CUDA],\n",
        "    schedule=torch.profiler.schedule(wait=1, warmup=1, active=3),\n",
        "    on_trace_ready=torch.profiler.tensorboard_trace_handler(profile_logdir),\n",
        "    record_shapes=False,\n",
        "    with_stack=False,\n",
        "    profile_memory=False\n",
        ") as prof:\n",
        "    for step in range(num_repeats):\n",
        "        start_event.record()\n",
        "\n",
        "        graph.replay()\n",
        "        torch.cuda.synchronize()\n",
        "        # Collect and save metrics\n",
        "        step_metrics = get_gpu_metrics()\n",
        "        metrics_per_step.append(step_metrics)\n",
        "\n",
        "        end_event.record()\n",
        "\n",
        "        elapsed_time_ms = start_event.elapsed_time(end_event)\n",
        "        total_time_ms += elapsed_time_ms\n",
        "\n",
        "        torch.cuda.empty_cache()\n",
        "        gc.collect()\n",
        "\n",
        "        prof.step()\n",
        "\n",
        "print(\"Profiling complete! ðŸš€\")\n",
        "\n",
        "avg_time_per_replay_ms = total_time_ms / num_repeats\n",
        "\n",
        "print(f\"\\\\nðŸš€ Benchmark Results:\")\n",
        "print(f\"Batch size = {batch_size}, Num Frames = {num_frames}\")\n",
        "print(f\"Total time for {num_repeats} replays: {total_time_ms:.2f} ms\")\n",
        "print(f\"Average time per replay: {avg_time_per_replay_ms:.2f} ms\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1RZ2gwz0u4nS"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Extract fields\n",
        "sm_utilization = [m['sm_utilization_percent'] for m in metrics_per_step]\n",
        "mem_utilization = [m['memory_utilization_percent'] for m in metrics_per_step]\n",
        "bandwidth_utilization = [m['memory_bandwidth_utilization_percent'] for m in metrics_per_step]\n",
        "\n",
        "# Plot\n",
        "plt.figure(figsize=(12,6))\n",
        "plt.plot(sm_utilization, label='SM Utilization (%)')\n",
        "plt.plot(mem_utilization, label='Memory Utilization (%)')\n",
        "plt.plot(bandwidth_utilization, label='Memory Bandwidth Usage (%)')\n",
        "plt.xlabel('Replay Step')\n",
        "plt.ylabel('Percentage (%)')\n",
        "plt.title('GPU Utilization Metrics Over Steps')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cfrF915imD0p"
      },
      "outputs": [],
      "source": [
        "! ls -lart {profile_logdir}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K3yHKfNJ9VmL"
      },
      "outputs": [],
      "source": [
        "#%load_ext tensorboard"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lFWUDMey8h64"
      },
      "outputs": [],
      "source": [
        "#%tensorboard --logdir {profile_logdir}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7sVUOHTL9SdB"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
